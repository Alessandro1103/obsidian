Date: 2024-05-07
Time: 10:16
Tags: #ComputerVision #Universit√†
Up: [[Computer Vision]] 

---
# Transformers in Vision

## Introduction

This is a schematic of a Transformer model architecture, starting from the bottom here is a detailed description of the components:
1. **Inputs**: raw data (like text or images)
2. **Input Embedding**: the inputs are converted into a format that model can work with more effectively, typically a high-dimensional space where similar inputs are closer together. 
3. **Positional Encoding**: are added to the input embeddings to provide the model with information about the order of the input elements. 

![[Screenshot from 2024-05-07 10-18-38.png|300]]

