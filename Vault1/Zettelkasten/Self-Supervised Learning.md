Date: 2024-05-07
Time: 11:09
Tags: #ComputerVision #Università 
Up: [[Computer Vision]]

---
# Self-Supervised Learning

![[Recording 20240507124348.webm]]

Data annotation is one of the main problem in Deep Learning, and in general in AI.

![[Screenshot from 2024-05-07 11-10-46.png|300]]

The time increase in complex problems, there could be some errors, generated by humans:
- **Fine-grained recognition**: errors in subcategorise classes (e.g.. species fo dogs).
- **Class unawareness**: the annotator isn't aware of all the possible subcategories in the data.
- **Insufficient training data**: small amount of data.

Rare concepts:
![[Screenshot from 2024-05-07 11-15-23.png|400]]


![[Screenshot from 2024-05-07 11-16-38.png]]

In case like this the annotation time is 90 minutes per image, with multiple annotators
So has become pretty likely that dataset are now sintetic, like stereo modepth and Optical Flow, which can be difficult task for humans:![[Screenshot from 2024-05-07 11-18-13.png]]


## In the context of Computer Vision

we watn to obtain the label from the data itself, extracting a property, that can help me classify it.

The task can be multimple:
- Predict anu part of the input form any other part
- Predict the future form the past
- Predict the future from the recent past
- Predict the past from the present
- Predict the top from the bottom
- Predict the occluded from the visible

An usage made for Supervised learning is to correct a corrupted version. After training, only the encoder is kept and the decoder is thrown away. This can be used for an another type of classification or maybe in other datasets.

![[Screenshot from 2024-05-07 11-21-48.png|500]]

Self-supervise learning learn model parameters using dataset of data-data pairs $\{x_i,x_i'\}^N_{i=1}$, like for example self sepervised stereo/flow, contrastive learning.


## Task-specific Models

### Unsupervised Learning of Depth and Ego-Motion

![[Screenshot from 2024-05-07 11-28-13.png|500]]

- **For Depth Estimation**: The Depth CNN predicts a depth map $D_t​(p)$ for each pixel $p$ in the target view $It$.
- **For Pose Estimation**: The Pose CNN predicts the transformations (rotation and translation) between the target view and each of the nearby views.
- **Projection**: Using the depth map from the Depth CNN and the transformations from the Pose CNN, points from the target view are projected into the source views. This step is crucial for ensuring that the depth and pose estimations make sense when comparing the target frame to its adjacent frames.
- **Source View Alignment**: By projecting the target view's depth into the source views based on the estimated pose, the model can verify if the predicted transformations and depth map are accurate. If the projection closely aligns with the actual source views, it indicates accurate predictions.
Train CNN to jointly predict depth and relative pose from three video frames. 

![[Screenshot from 2024-05-07 11-31-49.png|450]]

Traditional U-Net with multi-scale prediction/loss. Final objective includes photoconsistency, smoothness. 

**Monodepth from Stereo Supervision**:

we want to train a 

![[Screenshot from 2024-05-07 11-45-38.png]]


![[Screenshot from 2024-05-07 11-46-33.png]]

![[Screenshot from 2024-05-07 11-48-15.png]]


- **(a)** the depth network processes the input image and outputs a depth map $D_t$
- **(b)** the pose network estimates the camera motion between the current frame $(t)$ and other frames $(t')$. 
- **(c)** 
- **(d)** the better you see the image the better result you will have.


![[Screenshot from 2024-05-07 11-52-47.png]]

![[Screenshot from 2024-05-07 11-54-20.png]]


![[Screenshot from 2024-05-07 11-58-41.png]]

## Pretext tasks

![[Screenshot from 2024-05-07 12-00-18.png]]

I want to predict relative position of patches:
![[Screenshot from 2024-05-07 12-06-18.png]]

![[Screenshot from 2024-05-07 12-12-44.png]]

![[Screenshot from 2024-05-07 12-13-43.png]]

![[Screenshot from 2024-05-07 12-14-08.png]]

![[Screenshot from 2024-05-07 12-14-37.png]]![[Screenshot from 2024-05-07 12-15-23.png]]
We have to control even if the model cheats, we have to prevent shortcut learning. With shortcut learning we mean when machine learning model finds a simple solution to a problem that works well on the training data but fails to generalize to new, unseen data. Strategies to prevent this:
1. **Low level Statistics**: When adjacent patches in an image have similar low-level statistics (like mean and variance), a model might simply use these statistics to solve the puzzle rather than understanding the content of the image. The solution can be a simple normalization of the mean and variance.
2. **Edge Continuity**: Models can over-rely on the continuity of edges between pieces. The solution can be to select 64x64 pixel tiles randomly from slightly larger 85x85 pixel cells, disrupting direct edge continuity.
3. **Chromatic Aberration**: 

![[Screenshot from 2024-05-07 12-25-48.png]]

Other task:
- Inpainting task: try to recover a region.
- Rotation task: try to recover the true orientation.






## Contrastive Learning

