Date: 2024-05-07
Time: 11:09
Tags: #ComputerVision #Università 
Up: [[Computer Vision]]

---
# Self-Supervised Learning

## Data Annotation

Data annotation is one of the main problem in Deep Learning, and in general in AI.

![[Screenshot from 2024-05-07 11-10-46.png|300]]

The time increase in complex problems, there could be some errors, generated by humans:
- **Fine-grained recognition**: errors in subcategorise classes (e.g.. species fo dogs).
- **Class unawareness**: the annotator isn't aware of all the possible subcategories in the data.
- **Insufficient training data**: small amount of data.

**Class Imbalance**:
![[Screenshot from 2024-05-07 11-15-23.png|400]]

This is a situation where a small amount of classes takes the most part of the data, meaning that of some classes we don't have enough images.

**Dense semantic**:
![[Pasted image 20240508154133.png|500]]

In case like this the annotation time is 90 minutes per image, with multiple annotators. This because every pixel has to be correctly described.
So has become pretty likely that dataset are now *synthetic*, like stereo, monocular depth and Optical Flow, which can be difficult task for humans:
![[Screenshot from 2024-05-07 11-18-13.png|400]]


## Self-Supervision

**Definition**:
Self-Supervision is used where models use unlabeled data by learning to predict unseen or hidden parts of the data based on observed portions, trying to generate their own labels for training.

An usage made for Supervised learning is to correct a corrupted version. After training, only the encoder is kept and the decoder is thrown away. This can be used for an another type of classification or maybe in other datasets.

![[Screenshot from 2024-05-07 11-21-48.png|500]]

Self-Supervised learning learn model parameters using dataset of data-data pairs $\{x_i,x_i'\}^N_{i=1}$, like for example self supervised stereo/flow, contrastive learning.


## Task-specific Models

### Unsupervised Learning of Depth and Ego-Motion

![[Screenshot from 2024-05-07 11-28-13.png|500]]

- **For Depth Estimation**: The Depth CNN predicts a depth map $D_t​(p)$ for each pixel $p$ in the target view $It$.
- **For Pose Estimation**: The Pose CNN predicts the transformations (rotation and translation) between the target view and each of the nearby views.
- **Projection**: Using the depth map from the Depth CNN and the transformations from the Pose CNN, points from the target view are projected into the source views. This step is crucial for ensuring that the depth and pose estimations make sense when comparing the target frame to its adjacent frames.
- **Source View Alignment**: By projecting the target view's depth into the source views based on the estimated pose, the model can verify if the predicted transformations and depth map are accurate. If the projection closely aligns with the actual source views, it indicates accurate predictions.

Train CNN to jointly predict depth and relative pose from three video frames. 

![[Screenshot from 2024-05-07 11-31-49.png|450]]

Traditional U-Net with multi-scale prediction/loss. Final objective includes photoconsistency, smoothness. 

### Monodepth Estimation from Stereo Supervision:
In stereo depth estimation, "predicting" refers to calculating how an image appears from a different spatial perspective, not how it changes over time, using disparity maps to measure object distances and reconstruct different viewpoints. A disparity map measures the pixel differences between two stereo images, indicating the distance of objects from the viewer by their shift in position across the images.

![[Screenshot from 2024-05-07 11-45-38.png]]

The tree approaches can be summarized in:
- **Naive Approach**: the right image is predicted by sampling directly from the left image, creating a disparity map that aligns with the target right image.
- **No LR**: generates the left view by sampling from the right image. Can introduce 'texture-copy' artifacts and inaccuracies at depth discontinuities.
- **Ours**: involves training the network to predict disparity maps for both views by sampling from their opposite images, ensuring the network uses the right image only during training and not in deployment.

This is the training. In the test application only one image will be used.

### Digging Into Self-Supervised Monocular Depth Estimation

![[Screenshot from 2024-05-07 11-48-15.png]]

**Monodepth2**:
- **(a)** the depth network processes the input image and outputs a depth map $D_t$.
- **(b)** the pose network estimates the camera motion between two frames $(t,t')$. 
- **(c)** describes an advanced loss function used during the training of the depth estimation network. Instead use the "avg" operation, between forward and backward pixel reprojection errors, a "min" operation is able to better handle discrepancies.
- **(d)** the use of multiple scales helps in capturing both global structure and local details of the scene.

### Unsupervised Learning of Optical Flow

![[Screenshot from 2024-05-07 11-52-47.png|600]]

The image is self-explanatory

![[Screenshot from 2024-05-07 11-54-20.png|600]]

In this case is shown how we can obtain different type of error to understand better the neural: *data loss*, *consistency loss*

### Self-Supervised Monocular Scene Flow Estimation

![[Screenshot from 2024-05-07 11-58-41.png|600]]

## Pretext tasks

Pretext tasks are pre-designed tasks for networks to solve, and visual features are learned by learning objective functions of pretext tasks.

![[Screenshot from 2024-05-07 12-00-18.png|600]]

We have said that Self-Supervised learning is used to extract a feature too, well at the moment is the most important aspect of this type of machine learning. Extract the first part of the neural structure permits to adapt the neural to multiple tasks, alongside with a Supervised Learning with labelled data.


### Visual Representation Learning by Solving Jigsaw Puzzles

![[Screenshot from 2024-05-07 12-06-18.png|400]]


![[Screenshot from 2024-05-07 12-12-44.png|400]]


![[Screenshot from 2024-05-07 12-14-08.png|400]]


![[Screenshot from 2024-05-07 12-15-23.png|400]]

### Other tasks
- Inpainting task: try to recover a region.
- Rotation task: try to recover the true orientation.

### Cheaters
We have to control even if the model cheats, we have to prevent shortcut learning. With shortcut learning we mean when machine learning model finds a simple solution to a problem that works well on the training data but fails to generalize to new, unseen data. Strategies to prevent this:
1. **Low level Statistics**: When adjacent patches in an image have similar low-level statistics (like mean and variance), a model might simply use these statistics to solve the puzzle rather than understanding the content of the image. The solution can be a simple normalization of the mean and variance.
2. **Edge Continuity**: Models can over-rely on the continuity of edges between pieces. The solution can be to select 64x64 pixel tiles randomly from slightly larger 85x85 pixel cells, disrupting direct edge continuity.
3. **Chromatic Aberration**: It's a type of distortion in which there is a failure of a lens to focus all colors to the same convergence point (see the borders):
   ![[Screenshot from 2024-05-07 12-13-43.png|200]]

## Contrastive Learning

### Hope of Generalization

>[!Info]
In simple terms, pre-train a neural network refers to first training a model on one task or dataset. Then using the parameters or model from this training to train another model on a different task or dataset.

With Hope of Generalization in machine learning, specifically focusing on how CNN are pre-trained and how this pre-training can potentially be transferred to other tasks:

1. The process starts with large dataset (pre train data)
2. The ConvNet undergoes training using a specific task without supervision
3. The ConvNet's learned features are fixed, and only the final layer is trained to perform new task.

The "Hope of Generalization" is the expectation or hope that the network will learn universally useful features during pre-training that are applicable to a wide range of tasks.

>[!example] 
**Pre-training Task**: Let's say you train a neural network to recognize and rearrange shuffled parts of an image (like a Jigsaw puzzle).
**Transfer Task**: Later, you want to use the learned features from this network to perform object recognition.
**Alignment**: If the features learned during the Jigsaw puzzle task help the network recognize edges, shapes, and textures of objects, there is a good alignment. These features are useful for identifying objects in new images during the transfer task.

### Contrastive Learning

The pretext task should be invariant to nuisance factors (location lighting color).

![[Pasted image 20240510121400.png|500]]

Given a score function $s(\cdot,\cdot)$, we want to learn an encoder $f$ that yields "
- high score for positive pairs: $(x,x^+)$
- low score for negative pairs: $(x,x^-)$
$\rightarrow s(f(x), f(x^+)>>s(f(x), f(x^-))$

This encoder measure the similarities of images referred to a certain feature.
Let's consider a **multi-class cross entropy loss function**:
$$
\mathcal{L} = -\mathbb{E}_x \left[ \log \frac{\exp(s(f(x), f(x^+)))}{\exp(s(f(x), f(x^+))) + \sum_{j=1}^{N-1} \exp(s(f(x), f(x_j^-)))} \right]
$$

The loss function is designed to push the encoder $f$ to produce similar feature vector for $x,x^+$, and dissimilar feature vectors for $x$ and $x^-$.
Known as **InfoNCE loss** and its negative is a *lower bound* on the mutual information between $f(x)$ and $f(x^+)$:
$$
MI[f(x),f(x^+)]\geq log(N)-\mathcal{L}
$$
The score function is defined as:
$$
s(f_1, f_2) = \frac{f_1^T f_2}{||f_1|| \cdot ||f_2||}
$$
This choice is favoured because it effectively captures how aligned or similar two feature vectors are (measures cosine), regardless of their magnitude, making it ideal for comparing normalized embeddings.

SimCLR uses a projection network to project features to a space where constrastive learning is applied. The projection improves learning.

![[Pasted image 20240510140004.png|250]]

![[Pasted image 20240510140455.png|500]]

### Momentum Contrast

Momentum Contrast (MoCo) trains a visual representation encoder by matching an encoded query $q$ to a dictionary of encoded keys using a contrastive loss. The dictionary keys ${k0, k1, k2, \dots}$ are defined on-the-fly by a set of data samples. The dictionary is built as a queue, with the current mini-batch enqueued and the oldest mini-batch dequeued, decoupling it from the mini-batch size. The keys in the dictionary are encoded by a secondary encoder, which progresses more slowly than the primary or query encoder. This slower progression is maintained through a momentum-based update rule, where the parameters 𝜃𝑘θk​ of the key encoder are updated as follows: $\theta_k = \beta \theta_k + (1-\beta) \theta_q$
![[Pasted image 20240510144320.png|250]]


### Barlow Twins

The method approach is to produce 2 distorted views for all the images of a batch $X$ sampled from a dataset. After a deep network these produces batches of embeddings $Z^A, Z^B$. The loss function is innovative:
$$
\mathcal{L}_\mathcal{BT} = \sum_i (1 - c_{ii})^2 + \lambda\sum_i \sum_{j \neq i} c_{ij}^2

$$

Where $C$ is the cross-correlation matrix computed between the outputs of the two identical networks. 
![[Pasted image 20240510150609.png]]

There is no need of negative samples in contrastive learning