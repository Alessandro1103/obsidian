Date: 2024-05-07
Time: 11:09
Tags: #ComputerVision #Università 
Up: [[Computer Vision]]

---
# Self-Supervised Learning

![[Recording 20240507124348.webm]]

## Data Annotation

Data annotation is one of the main problem in Deep Learning, and in general in AI.

![[Screenshot from 2024-05-07 11-10-46.png|300]]

The time increase in complex problems, there could be some errors, generated by humans:
- **Fine-grained recognition**: errors in subcategorise classes (e.g.. species fo dogs).
- **Class unawareness**: the annotator isn't aware of all the possible subcategories in the data.
- **Insufficient training data**: small amount of data.

**Class Imbalance**:
![[Screenshot from 2024-05-07 11-15-23.png|400]]

This is a situation where a small amount of classes takes the most part of the data, meaning that of some classes we don't have enough images.

**Dense semantic**:
![[Pasted image 20240508154133.png|500]]

In case like this the annotation time is 90 minutes per image, with multiple annotators. This because every pixel has to be correctly described.
So has become pretty likely that dataset are now *synthetic*, like stereo, monocular depth and Optical Flow, which can be difficult task for humans:
![[Screenshot from 2024-05-07 11-18-13.png|400]]


## Self-Supervision

**Definition**:
Self-Supervision is used where models use unlabeled data by learning to predict unseen or hidden parts of the data based on observed portions, trying to generate their own labels for training.

An usage made for Supervised learning is to correct a corrupted version. After training, only the encoder is kept and the decoder is thrown away. This can be used for an another type of classification or maybe in other datasets.

![[Screenshot from 2024-05-07 11-21-48.png|500]]

Self-Supervised learning learn model parameters using dataset of data-data pairs $\{x_i,x_i'\}^N_{i=1}$, like for example self supervised stereo/flow, contrastive learning.


## Task-specific Models

### Unsupervised Learning of Depth and Ego-Motion

![[Screenshot from 2024-05-07 11-28-13.png|500]]

- **For Depth Estimation**: The Depth CNN predicts a depth map $D_t​(p)$ for each pixel $p$ in the target view $It$.
- **For Pose Estimation**: The Pose CNN predicts the transformations (rotation and translation) between the target view and each of the nearby views.
- **Projection**: Using the depth map from the Depth CNN and the transformations from the Pose CNN, points from the target view are projected into the source views. This step is crucial for ensuring that the depth and pose estimations make sense when comparing the target frame to its adjacent frames.
- **Source View Alignment**: By projecting the target view's depth into the source views based on the estimated pose, the model can verify if the predicted transformations and depth map are accurate. If the projection closely aligns with the actual source views, it indicates accurate predictions.

Train CNN to jointly predict depth and relative pose from three video frames. 

![[Screenshot from 2024-05-07 11-31-49.png|450]]

Traditional U-Net with multi-scale prediction/loss. Final objective includes photoconsistency, smoothness. 

**Monodepth Estimation from Stereo Supervision**:

![[Screenshot from 2024-05-07 11-45-38.png]]

The tree approaches can be summarized in:
- **Naive Approach**: generates a right image prediction (as final result) by sampling from the left image. Then create a disparity for the right image. The disparities are not aligned with the left image from which they are derived, which is not optimal for depth estimation tasks.
- **No LR**: wants to create a left view aligned disparity, b

![[Screenshot from 2024-05-07 11-46-33.png]]

![[Screenshot from 2024-05-07 11-48-15.png]]


- **(a)** the depth network processes the input image and outputs a depth map $D_t$
- **(b)** the pose network estimates the camera motion between the current frame $(t)$ and other frames $(t')$. 
- **(c)** 
- **(d)** the better you see the image the better result you will have.


![[Screenshot from 2024-05-07 11-52-47.png]]

![[Screenshot from 2024-05-07 11-54-20.png]]


![[Screenshot from 2024-05-07 11-58-41.png]]

## Pretext tasks

![[Screenshot from 2024-05-07 12-00-18.png]]

I want to predict relative position of patches:
![[Screenshot from 2024-05-07 12-06-18.png]]

![[Screenshot from 2024-05-07 12-12-44.png]]

![[Screenshot from 2024-05-07 12-13-43.png]]

![[Screenshot from 2024-05-07 12-14-08.png]]

![[Screenshot from 2024-05-07 12-14-37.png]]![[Screenshot from 2024-05-07 12-15-23.png]]
We have to control even if the model cheats, we have to prevent shortcut learning. With shortcut learning we mean when machine learning model finds a simple solution to a problem that works well on the training data but fails to generalize to new, unseen data. Strategies to prevent this:
1. **Low level Statistics**: When adjacent patches in an image have similar low-level statistics (like mean and variance), a model might simply use these statistics to solve the puzzle rather than understanding the content of the image. The solution can be a simple normalization of the mean and variance.
2. **Edge Continuity**: Models can over-rely on the continuity of edges between pieces. The solution can be to select 64x64 pixel tiles randomly from slightly larger 85x85 pixel cells, disrupting direct edge continuity.
3. **Chromatic Aberration**: 

![[Screenshot from 2024-05-07 12-25-48.png]]

Other task:
- Inpainting task: try to recover a region.
- Rotation task: try to recover the true orientation.






## Contrastive Learning

